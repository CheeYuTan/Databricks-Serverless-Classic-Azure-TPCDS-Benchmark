{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad999e06-3441-4fd0-8396-c13629f41492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#TPC-DS data generator to run as a job\n",
    "\n",
    "## How to use it\n",
    "1. Attach to a cluster with \"Dedicated\" Access Mode that you have \"can manage\" permission.\n",
    "2. Run \"Cmd 2\". This will create a drop down box for you to choose various Scale Factor for the TPC-DS data generation.\n",
    "3. Pick the desired scale factor and the catalog in which the database will be created.\n",
    "4. Larger scale factor will take longer to complete. We recommend attaching to the following cluster spec for each scale factor.\n",
    "5. Always use fixed sized cluster; do not use Autoscaling for the cluster. Use Dedicated Access Mode!\n",
    "<pre>\n",
    "   Scale Factor      Cluster Spec (worker spec i3.2xlarge for AWS; Standard_E8ds_v4 for Azure)\n",
    "   1                 Worker:  4 workers\n",
    "   10                Worker:  4 workers\n",
    "   100               Worker:  8 workers\n",
    "   1000              Worker: 16 workers\n",
    "   10000             Worker: 32 workers\n",
    "</pre>\n",
    " 5. Click the \"Run All\"\n",
    " 6. Once the notebook finishes, you should see the database tpcds_sf<scale factor>_delta in the Data Explorer\n",
    "  \n",
    "### Troubleshooting\n",
    "1. If something went wrong and you need to do a clean data generation, switch \"Overwrite Data\" to true and then click \"Run All\".\n",
    "\n",
    "### IMPORTANT\n",
    "1. This datagen notebook will create tables in a schema named `tpcds_sfX_delta` (where `X` is the scaling factor that you selected). Make sure you have permission to create a schema/database in the target catalog. If you run with `Overwrite Data` of true, the notebook will drop that database if it exists.\n",
    "2. Make sure to use the latest [Databricks JDBC driver](https://www.databricks.com/spark/jdbc-drivers-download) to run the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7093a66e-bbcd-43d5-8d35-f83ba3c6c852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8ad11e-8441-4bff-9204-062a873c40fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.widgets.dropdown(\"scaleFactors\", \"1\", [\"1\", \"10\", \"100\", \"1000\", \"10000\"], \"Scale Factor\")\n",
    "dbutils.widgets.dropdown(\"overwrite\", \"false\", [\"true\", \"false\"], \"Overwrite Data\")\n",
    "dbutils.widgets.text(\"catalog\", \"main\", \"catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627da0aa-e05d-4ec1-8246-923c02275d34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the job parameters or use default values"
    }
   },
   "outputs": [],
   "source": [
    "// Multi TPC- H and DS generator and database importer using spark-sql-perf, typically to generate parquet files in S3/blobstore objects\n",
    "def tryGetWidget[T](name: String): scala.util.Try[String] = scala.util.Try(dbutils.widgets.get(name))\n",
    "def tryGetWidgetSet[T](name: String): scala.util.Try[Set[String]] = scala.util.Try(dbutils.widgets.get(name).split(\",\").map(_.trim).toSet)\n",
    "\n",
    "/* for Shasta\n",
    "val expectedWorkers = tryGetWidget(\"expectedWorkers\").getOrElse(\"8\")\n",
    "spark.conf.set(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\", expectedWorkers) */\n",
    "\n",
    "val benchmarks = tryGetWidgetSet(\"benchmarks\").getOrElse(Set(\"TPCDS\")).map(_.toUpperCase)\n",
    "val scaleFactors = tryGetWidgetSet(\"scaleFactors\").getOrElse(Set(\"1\")).map(_.toInt).toSeq.sorted.map(_.toString) \n",
    "\n",
    "val baseDatagenFolder = tryGetWidget(\"baseDatagenFolder\").getOrElse(\"/local_disk0/tmp\")  // usually /tmp if enough space is available for datagen files\n",
    "\n",
    "// Output files\n",
    "val overwrite = tryGetWidget(\"overwrite\").map(_.toBoolean).getOrElse(true) //if to delete existing files (doesn't check if results are complete on no-overwrite)\n",
    "val fileFormat = tryGetWidget(\"fileFormat\").getOrElse(\"delta\") // parquet, delta, orc, etc\n",
    "val clusterTables = tryGetWidget(\"clusterTables\").map(_.toBoolean).getOrElse(true) // if to liquid-cluster tables\n",
    "val distributeStrategy = tryGetWidget(\"distributeStrategy\").getOrElse(\n",
    "  if (fileFormat == \"delta\") \"none\" else \"distributeBy\") // experimental: none, distributeBy, clusterBy, packBy\n",
    "val coalesceInto: Int = tryGetWidget(\"coalesceInto\").map(_.toInt).getOrElse(1) // For non-delta, how many files for non-partitioned tables.  This determines parallelism in the writes\n",
    "\n",
    "\n",
    "// Generate stats for CBO\n",
    "val createTableStats = tryGetWidget(\"createTableStats\").map(_.toBoolean).getOrElse(true)\n",
    "val createColumnStats = tryGetWidget(\"createColumnStats\").map(_.toBoolean).getOrElse(true)\n",
    "\n",
    "val workers: Int = if (spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\").toInt > 0) spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\").toInt else 1 //number of nodes, assumes one executor per node.  \n",
    "val cores: Int = Runtime.getRuntime.availableProcessors.toInt //number of CPU-cores for parallelization calculation\n",
    "\n",
    "// Set only if creating multiple DBs or source file folders with different settings, use a leading _\n",
    "var dbSuffix = tryGetWidget(\"dbSuffix\").getOrElse(\"\") \n",
    "if (!clusterTables) dbSuffix = \"_nocluster\" + dbSuffix\n",
    "if (!createTableStats) dbSuffix = \"_nostats\" + dbSuffix\n",
    "\n",
    "// Set to generate file and schema naming and datatypes compatible with older results (legacy)\n",
    "// as in: tpcds/sf1000-parquet/useDecimal=false,useDate=false,filterNull=false\n",
    "val TPCDSUseLegacyOptions = tryGetWidget(\"TPCDSUseLegacyOptions\").map(_.toBoolean).getOrElse(false) \n",
    "\n",
    "val TPCDSUseDoubleForDecimal = tryGetWidget(\"TPCDSUseDoubleForDecimal\").map(_.toBoolean).getOrElse(false)\n",
    "if (TPCDSUseDoubleForDecimal) dbSuffix = \"_nodecimal\" + dbSuffix\n",
    "\n",
    "val duplicateTPCDScolumn = tryGetWidget(\"duplicateTPCDScolumn\").map(_.toBoolean).getOrElse(true)  // to accomodate both new and older spec version queries (ie., q30)\n",
    "\n",
    "val skipOptimize = tryGetWidget(\"skipOptimize\").map(_.toBoolean).getOrElse(false)  // to skip the delta optimize ie., for manual tests\n",
    "val skipChecks = tryGetWidget(\"skipChecks\").map(_.toBoolean).getOrElse(true)  // to skip some slow check ie., for running them in a separate job\n",
    "\n",
    "//val parallelizeTables = tryGetWidget(\"parallelizeTables\").map(_.toBoolean).getOrElse(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e4526a8-eac6-47ad-b88b-e29a35801270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val catalog = tryGetWidget(\"catalog\").getOrElse(\"main\")\n",
    "sql(s\"use catalog $catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beec280a-ae34-4fdc-a80b-acd8486547d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the core count from the executors and add one in case of odd numbers ES-38338"
    }
   },
   "outputs": [],
   "source": [
    "import scala.concurrent.duration._\n",
    "var (_, coresStr) = sc.runOnEachExecutor[String](() => {\n",
    " Runtime.getRuntime.availableProcessors.toString\n",
    "}, 5.seconds).mapValues[String](t => t.get).take(1).head\n",
    "\n",
    "val cores = if (coresStr.toInt % 2 == 0) coresStr.toInt else coresStr.toInt +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d86a102-8a71-443b-bf58-fdae2d321a8b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "// Imports, fail fast if we are missing any library\n",
    "\n",
    "// For datagens\n",
    "import java.io._\n",
    "import scala.sys.process._\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Spark/Hadoop config\n",
    "import org.apache.spark.deploy.SparkHadoopUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdbc0f4-c96c-49ba-91e9-2d20b2979bee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Spark settings for writing data"
    }
   },
   "outputs": [],
   "source": [
    "// Set Spark config to produce same and comparable source files across runs\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", (cores * workers * 2).toString) // 2 writers per cluster cores\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", \"0\")  // force larger files\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.charVarcharAsString\", \"true\") // needed for 8.x+\n",
    "\n",
    "// S3 \n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.multipart.size\", \"536870912\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "\n",
    "\n",
    "if (Seq(\"delta\", \"tahoe\").contains(fileFormat) && clusterTables) {\n",
    "  spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "//   spark.conf.set(\"spark.databricks.delta.optimizeWrite.binSize\", \"4096\")\n",
    "//   spark.conf.set(\"spark.databricks.delta.optimizeWrite.numShuffleBlocks\", \"5000000\")\n",
    "} else {\n",
    "  // add parquet configs here\n",
    "}\n",
    "\n",
    "if (fileFormat == \"orc\") {\n",
    "  spark.sqlContext.setConf(\"spark.sql.orc.impl\", \"native\")\n",
    "  spark.sqlContext.setConf(\"spark.sql.orc.enableVectorizedReader\", \"true\") \n",
    "  spark.sqlContext.setConf(\"spark.sql.hive.convertMetastoreOrc\", \"true\")\n",
    "  spark.sqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\")\n",
    "  spark.sqlContext.setConf(\"spark.sql.orc.char.enabled\", \"true\")\n",
    "  spark.sqlContext.setConf(\"spark.sql.orc.compression.codec\", \"snappy\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d5d369-5d79-4678-b8fe-8a6c8bfa2994",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Settings by scale factor"
    }
   },
   "outputs": [],
   "source": [
    "def setScaleConfig(scaleFactor: String): Unit = {\n",
    "  if (scaleFactor.toInt >= 100000) { \n",
    "    SparkHadoopUtil.get.conf.set(\"parquet.memory.pool.ratio\", \"0.05\")\n",
    "  }   \n",
    "  else if (scaleFactor.toInt >= 10000) {    \n",
    "    SparkHadoopUtil.get.conf.set(\"parquet.memory.pool.ratio\", \"0.1\")\n",
    "  } \n",
    "  else if (scaleFactor.toInt >= 1000) {\n",
    "    SparkHadoopUtil.get.conf.set(\"parquet.memory.pool.ratio\", \"0.3\")    \n",
    "  }\n",
    "  else { \n",
    "    SparkHadoopUtil.get.conf.set(\"parquet.memory.pool.ratio\", \"0.5\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bafefd-9fef-49f9-9be2-ffef09c39f17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logger and utilities"
    }
   },
   "outputs": [],
   "source": [
    "//import org.apache.log4j.Level\n",
    "\n",
    "//@transient lazy val logger = org.apache.log4j.LogManager.getLogger(s\"Notebook-logger\")\n",
    "\n",
    "def log(str: String) = {\n",
    "  println(java.time.LocalDateTime.now + s\"\\t${str}\")\n",
    "  //logger.info(s\"${str}\")\n",
    "}\n",
    "\n",
    "// Time command helper\n",
    "var timings = scala.collection.mutable.Map[String, Long]()\n",
    "def time[R](blockName: String, block: => R): R = {  \n",
    "    log(s\"Starting '$blockName'...\")\n",
    "    val t0 = System.currentTimeMillis() //nanoTime()\n",
    "    val result = block    // call-by-name\n",
    "    val t1 = System.currentTimeMillis() //nanoTime()\n",
    "    val elapsed = t1 -t0\n",
    "    timings += (blockName -> elapsed)\n",
    "    log(s\"Elapsed time for '$blockName': $elapsed ms\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d07107-6c5e-40e5-a80d-7b70c0e4eb1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wait for all the workers to be ready, we need to install the tpc kits"
    }
   },
   "outputs": [],
   "source": [
    "// Checks that we have the correct number of worker nodes to start the data generation\n",
    "// Make sure you have set the workers variable correctly, as the datagens binaries need to be present in all nodes\n",
    "val targetWorkers: Int = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\").toInt\n",
    "def numWorkers: Int = sc.getExecutorMemoryStatus.size - 1\n",
    "def waitForWorkers(requiredWorkers: Int, tries: Int) : Unit = {\n",
    "  for (i <- 0 to (tries-1)) {\n",
    "    if (numWorkers == requiredWorkers) {\n",
    "      log(s\"All workers ready. Waited ${i}s. for $numWorkers workers to be ready.\")\n",
    "      return\n",
    "    }\n",
    "    if (i % 60 == 0) println(s\"waiting ${i}s. for workers to be ready, got only $numWorkers workers\")\n",
    "    Thread sleep 1000\n",
    "  }\n",
    "  val failedMsg = s\"Timed out waiting for workers to be ready after ${tries}s.\"\n",
    "  log(failedMsg)\n",
    "  throw new Exception(failedMsg)\n",
    "}\n",
    "waitForWorkers(targetWorkers, 3600) //wait up to an hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba1c2cd-49ea-4522-a77f-1a913b9b15e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installer for TPC-H (dbgen)"
    }
   },
   "outputs": [],
   "source": [
    "// FOR INSTALLING TPCH DBGEN (with the sdtout patch)\n",
    "def installDBGEN(url: String = \"https://github.com/databricks/tpch-dbgen.git\", useStdout: Boolean = true, baseFolder: String = \"/tmp\")(i: java.lang.Long): String = {\n",
    "  // check if we want the revision which makes dbgen output to stdout\n",
    "  val checkoutRevision: String = if (useStdout) \"git checkout 0469309147b42abac8857fa61b4cf69a6d3128a8 -- bm_utils.c\" else \"\"\n",
    "  Seq(\"mkdir\", \"-p\", baseFolder).!\n",
    "  val pw = new PrintWriter(new File(s\"${baseFolder}/dbgen_$i.sh\" ))\n",
    "  pw.write(s\"\"\"\n",
    "rm -rf ${baseFolder}/dbgen\n",
    "rm -rf ${baseFolder}/dbgen_install_$i\n",
    "mkdir ${baseFolder}/dbgen_install_$i\n",
    "cd ${baseFolder}/dbgen_install_$i\n",
    "git clone '$url'\n",
    "cd tpch-dbgen\n",
    "$checkoutRevision\n",
    "make\n",
    "ln -sf ${baseFolder}/dbgen_install_$i/tpch-dbgen ${baseFolder}/dbgen || echo \"ln -sf failed\"\n",
    "test -e ${baseFolder}/dbgen/dbgen\n",
    "echo \"OK\"\n",
    "  \"\"\")\n",
    "  pw.close\n",
    "  Seq(\"chmod\", \"+x\", s\"${baseFolder}/dbgen_$i.sh\").!\n",
    "  Seq(s\"${baseFolder}/dbgen_$i.sh\").!!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d308d24a-11e4-486e-80d9-4969a2f81571",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Installer for TPC-DS (dsdgen)"
    }
   },
   "outputs": [],
   "source": [
    "// FOR INSTALLING TPCDS DSDGEN (with the sdtout patch) \n",
    "// Note: it assumes Debian/Ubuntu host, edit package manager if not\n",
    "def installDSDGEN(url: String = \"https://github.com/databricks/tpcds-kit.git\", useStdout: Boolean = true, baseFolder: String = \"/tmp\")(i: java.lang.Long): String = {\n",
    "  Seq(\"mkdir\", \"-p\", baseFolder).!\n",
    "  val pw = new PrintWriter(new File(s\"${baseFolder}/dsdgen_$i.sh\" ))\n",
    "  pw.write(s\"\"\"\n",
    "sudo apt-get update\n",
    "sudo apt-get -y --force-yes install gcc make flex bison byacc git\n",
    "rm -rf ${baseFolder}/dsdgen\n",
    "rm -rf ${baseFolder}/dsdgen_install_$i\n",
    "mkdir ${baseFolder}/dsdgen_install_$i\n",
    "cd ${baseFolder}/dsdgen_install_$i\n",
    "git clone '$url'\n",
    "cd tpcds-kit/tools\n",
    "make -f Makefile.suite\n",
    "ln -sf ${baseFolder}/dsdgen_install_$i/tpcds-kit/tools ${baseFolder}/dsdgen || echo \"ln -sf failed\"\n",
    "${baseFolder}/dsdgen/dsdgen -h\n",
    "test -e ${baseFolder}/dsdgen/dsdgen\n",
    "echo \"OK\"\n",
    "  \"\"\")\n",
    "  pw.close\n",
    "  Seq(\"chmod\", \"+x\", s\"${baseFolder}/dsdgen_$i.sh\").!\n",
    "  Seq(s\"${baseFolder}/dsdgen_$i.sh\").!!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb098809-cef1-4fa9-8cea-08cec4618040",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build and install TPC binaries in executors"
    }
   },
   "outputs": [],
   "source": [
    "// install (build) the data generators in all nodes\n",
    "val res = spark.range(0, workers, 1, workers).map(worker => benchmarks.map{\n",
    "    case \"TPCDS\" => s\"TPCDS worker $worker\\n\" + installDSDGEN(baseFolder = baseDatagenFolder)(worker)\n",
    "    case \"TPCH\" => s\"TPCH worker $worker\\n\" + installDBGEN(baseFolder = baseDatagenFolder)(worker)\n",
    "  }).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84e3df1-d9fd-4bfb-8ddd-ef15799e7cb7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set the schema name (ie., legacy configs), tables, and location for each benchmark"
    }
   },
   "outputs": [],
   "source": [
    "def getBenchmarkData(benchmark: String, scaleFactor: String) = benchmark match {    \n",
    "  case \"TPCH\" => (\n",
    "    s\"tpch_sf${scaleFactor}_${fileFormat}${dbSuffix}\")  \n",
    "  \n",
    "  case \"TPCDS\" if !TPCDSUseLegacyOptions && !TPCDSUseDoubleForDecimal  => (\n",
    "    s\"tpcds_sf${scaleFactor}_${fileFormat}${dbSuffix}\")\n",
    "  \n",
    "  case \"TPCDS\" if TPCDSUseDoubleForDecimal => (\n",
    "    s\"tpcds_sf${scaleFactor}_${fileFormat}${dbSuffix}\")  \n",
    "  \n",
    "  case \"TPCDS\" if TPCDSUseLegacyOptions => (\n",
    "    if (Seq(\"delta\", \"tahoe\").contains(fileFormat)) s\"tpcds_sf${scaleFactor}_nodecimal_nodate_withnulls_delta${dbSuffix}\" \n",
    "    else s\"tpcds_sf${scaleFactor}_nodecimal_nodate_withnulls${dbSuffix}\")\n",
    "}\n",
    "\n",
    "\n",
    "def getNameLocation(benchmark: String, scaleFactor: String) = getBenchmarkData(benchmark, scaleFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346a472a-a42c-4682-add0-c69cef1b14f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Utility functions"
    }
   },
   "outputs": [],
   "source": [
    "def getTables(db: String) = {\n",
    "  sql(s\"use `$db`\")\n",
    "  sql(s\"show tables\").select(\"tableName\")\n",
    "    .collect().map(_.toString.drop(1).dropRight(1)) \n",
    "}\n",
    "\n",
    "def getColumns(db: String, tableName: String) = table(s\"`$db`.`$tableName`\").columns\n",
    "\n",
    "def clusteredCols(tableName: String): Set[String] = {\n",
    "  try {\n",
    "    // Run DESCRIBE to get the table metadata including clustering info.\n",
    "    val descDF = sql(s\"DESCRIBE $tableName\")\n",
    "    // Collect the first column (col_name) into a List.\n",
    "    val rows: List[String] = descDF.collect().map(_.getString(0)).toList\n",
    "    // Find the marker row indicating the start of clustering info.\n",
    "    val markerIndex = rows.indexWhere(_.startsWith(\"# Clustering Information\"))\n",
    "    if (markerIndex == -1) {\n",
    "      // If no clustering section is present, return an empty set.\n",
    "      Set.empty[String]\n",
    "    } else {\n",
    "      // Skip the marker row and the next header row.\n",
    "      val clusteringRows = rows.drop(markerIndex + 2).takeWhile(row => row.nonEmpty && !row.startsWith(\"#\"))\n",
    "      clusteringRows.toSet\n",
    "    }\n",
    "  } catch {\n",
    "    case _: Throwable => Set.empty[String]\n",
    "  }\n",
    "} \n",
    "\n",
    "def clusteringString(tableName: String): String = {\n",
    "  val pCols = clusteredCols(tableName: String)\n",
    "  if (!pCols.isEmpty) \"CLUSTER BY (\" + pCols.mkString + \")\"\n",
    "  else \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a399cdb-6ce3-4e91-a2cf-dae3e99bed3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Expected tables for TPC- DS and H"
    }
   },
   "outputs": [],
   "source": [
    "val tableNamesTpcds = Seq(\n",
    "  \"inventory\", \"catalog_returns\", \"store_returns\",  \"web_returns\", \"web_sales\",  \"store_sales\", // with partitions\n",
    "  \"call_center\", \"catalog_page\", \n",
    "  \"customer_address\", \"customer_demographics\", \"customer\", \n",
    "  \"date_dim\", \"household_demographics\", \"income_band\",\n",
    "  \"item\", \"promotion\", \"reason\", \"ship_mode\", \n",
    "  \"store\", \"time_dim\", \n",
    "  \"warehouse\", \"web_page\", \n",
    "  \"web_site\",\n",
    "  \"catalog_sales\"\n",
    ").sorted\n",
    "\n",
    "// var tpcdsLargeTables = Set(\"customer_demographics\",\"customer_address\",\"customer\",\"inventory\",\"web_sales\",\"store_sales\",\"catalog_sales\")\n",
    "// var tpcdsSmallTables = Set(\"call_center\",\"catalog_page\",\"date_dim\",\"household_demographics\",\"income_band\",\"item\",\"promotion\",\"reason\",\"ship_mode\",\"store\",\"time_dim\",\n",
    "//                            \"warehouse\",\"web_page\",\"web_site\")\n",
    "\n",
    "val tableNamesTpch = Seq(\n",
    "  \"customer\", \"lineitem\", \"nation\", \"orders\", \"part\", \n",
    "  \"region\", \"supplier\", \"partsupp\"\n",
    ").sorted\n",
    "\n",
    "def getBenchmarkTables(benchmark: String) = benchmark match {\n",
    "  case \"TPCDS\" => tableNamesTpcds\n",
    "  case \"TPCH\" => tableNamesTpch\n",
    "  case _ => throw new Exception(s\"Invalid benchmark $benchmark\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a578250b-0148-4116-9085-425d1d69b161",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schemas for TPC- DS and H"
    }
   },
   "outputs": [],
   "source": [
    "val tableColumnSchemas = Map(\n",
    "\"dbgen_version\" -> \"\"\"\n",
    "    dv_version                varchar(16)                   ,\n",
    "    dv_create_date            date                          ,\n",
    "    dv_create_time            time                          ,\n",
    "    dv_cmdline_args           varchar(200)                  \n",
    "\"\"\",\n",
    "\"call_center\" -> \"\"\"\n",
    "    cc_call_center_sk         integer               not null,\n",
    "    cc_call_center_id         char(16)              not null,\n",
    "    cc_rec_start_date         date                          ,\n",
    "    cc_rec_end_date           date                          ,\n",
    "    cc_closed_date_sk         integer                       ,\n",
    "    cc_open_date_sk           integer                       ,\n",
    "    cc_name                   varchar(50)                   ,\n",
    "    cc_class                  varchar(50)                   ,\n",
    "    cc_employees              integer                       ,\n",
    "    cc_sq_ft                  integer                       ,\n",
    "    cc_hours                  char(20)                      ,\n",
    "    cc_manager                varchar(40)                   ,\n",
    "    cc_mkt_id                 integer                       ,\n",
    "    cc_mkt_class              char(50)                      ,\n",
    "    cc_mkt_desc               varchar(100)                  ,\n",
    "    cc_market_manager         varchar(40)                   ,\n",
    "    cc_division               integer                       ,\n",
    "    cc_division_name          varchar(50)                   ,\n",
    "    cc_company                integer                       ,\n",
    "    cc_company_name           char(50)                      ,\n",
    "    cc_street_number          char(10)                      ,\n",
    "    cc_street_name            varchar(60)                   ,\n",
    "    cc_street_type            char(15)                      ,\n",
    "    cc_suite_number           char(10)                      ,\n",
    "    cc_city                   varchar(60)                   ,\n",
    "    cc_county                 varchar(30)                   ,\n",
    "    cc_state                  char(2)                       ,\n",
    "    cc_zip                    char(10)                      ,\n",
    "    cc_country                varchar(20)                   ,\n",
    "    cc_gmt_offset             decimal(5,2)                  ,\n",
    "    cc_tax_percentage         decimal(5,2)                  \n",
    "\"\"\",\n",
    "\"catalog_page\" -> \"\"\"\n",
    "    cp_catalog_page_sk        integer               not null,\n",
    "    cp_catalog_page_id        char(16)              not null,\n",
    "    cp_start_date_sk          integer                       ,\n",
    "    cp_end_date_sk            integer                       ,\n",
    "    cp_department             varchar(50)                   ,\n",
    "    cp_catalog_number         integer                       ,\n",
    "    cp_catalog_page_number    integer                       ,\n",
    "    cp_description            varchar(100)                  ,\n",
    "    cp_type                   varchar(100)                  \n",
    "\"\"\",\n",
    "\"catalog_returns\" -> \"\"\"\n",
    "    cr_returned_date_sk       integer                       ,\n",
    "    cr_returned_time_sk       integer                       ,\n",
    "    cr_item_sk                integer               not null,\n",
    "    cr_refunded_customer_sk   integer                       ,\n",
    "    cr_refunded_cdemo_sk      integer                       ,\n",
    "    cr_refunded_hdemo_sk      integer                       ,\n",
    "    cr_refunded_addr_sk       integer                       ,\n",
    "    cr_returning_customer_sk  integer                       ,\n",
    "    cr_returning_cdemo_sk     integer                       ,\n",
    "    cr_returning_hdemo_sk     integer                       ,\n",
    "    cr_returning_addr_sk      integer                       ,\n",
    "    cr_call_center_sk         integer                       ,\n",
    "    cr_catalog_page_sk        integer                       ,\n",
    "    cr_ship_mode_sk           integer                       ,\n",
    "    cr_warehouse_sk           integer                       ,\n",
    "    cr_reason_sk              integer                       ,\n",
    "    cr_order_number           bigint                not null,\n",
    "    cr_return_quantity        integer                       ,\n",
    "    cr_return_amount          decimal(7,2)                  ,\n",
    "    cr_return_tax             decimal(7,2)                  ,\n",
    "    cr_return_amt_inc_tax     decimal(7,2)                  ,\n",
    "    cr_fee                    decimal(7,2)                  ,\n",
    "    cr_return_ship_cost       decimal(7,2)                  ,\n",
    "    cr_refunded_cash          decimal(7,2)                  ,\n",
    "    cr_reversed_charge        decimal(7,2)                  ,\n",
    "    cr_store_credit           decimal(7,2)                  ,\n",
    "    cr_net_loss               decimal(7,2)                  \n",
    "\"\"\",\n",
    "\"catalog_sales\" -> \"\"\"\n",
    "    cs_sold_date_sk           integer                       ,\n",
    "    cs_sold_time_sk           integer                       ,\n",
    "    cs_ship_date_sk           integer                       ,\n",
    "    cs_bill_customer_sk       integer                       ,\n",
    "    cs_bill_cdemo_sk          integer                       ,\n",
    "    cs_bill_hdemo_sk          integer                       ,\n",
    "    cs_bill_addr_sk           integer                       ,\n",
    "    cs_ship_customer_sk       integer                       ,\n",
    "    cs_ship_cdemo_sk          integer                       ,\n",
    "    cs_ship_hdemo_sk          integer                       ,\n",
    "    cs_ship_addr_sk           integer                       ,\n",
    "    cs_call_center_sk         integer                       ,\n",
    "    cs_catalog_page_sk        integer                       ,\n",
    "    cs_ship_mode_sk           integer                       ,\n",
    "    cs_warehouse_sk           integer                       ,\n",
    "    cs_item_sk                integer               not null,\n",
    "    cs_promo_sk               integer                       ,\n",
    "    cs_order_number           bigint                not null,\n",
    "    cs_quantity               integer                       ,\n",
    "    cs_wholesale_cost         decimal(7,2)                  ,\n",
    "    cs_list_price             decimal(7,2)                  ,\n",
    "    cs_sales_price            decimal(7,2)                  ,\n",
    "    cs_ext_discount_amt       decimal(7,2)                  ,\n",
    "    cs_ext_sales_price        decimal(7,2)                  ,\n",
    "    cs_ext_wholesale_cost     decimal(7,2)                  ,\n",
    "    cs_ext_list_price         decimal(7,2)                  ,\n",
    "    cs_ext_tax                decimal(7,2)                  ,\n",
    "    cs_coupon_amt             decimal(7,2)                  ,\n",
    "    cs_ext_ship_cost          decimal(7,2)                  ,\n",
    "    cs_net_paid               decimal(7,2)                  ,\n",
    "    cs_net_paid_inc_tax       decimal(7,2)                  ,\n",
    "    cs_net_paid_inc_ship      decimal(7,2)                  ,\n",
    "    cs_net_paid_inc_ship_tax  decimal(7,2)                  ,\n",
    "    cs_net_profit             decimal(7,2)                  \n",
    "\"\"\",\n",
    "\"customer\" -> \"\"\"\n",
    "    c_customer_sk             integer               not null,\n",
    "    c_customer_id             char(16)              not null,\n",
    "    c_current_cdemo_sk        integer                       ,\n",
    "    c_current_hdemo_sk        integer                       ,\n",
    "    c_current_addr_sk         integer                       ,\n",
    "    c_first_shipto_date_sk    integer                       ,\n",
    "    c_first_sales_date_sk     integer                       ,\n",
    "    c_salutation              char(10)                      ,\n",
    "    c_first_name              char(20)                      ,\n",
    "    c_last_name               char(30)                      ,\n",
    "    c_preferred_cust_flag     char(1)                       ,\n",
    "    c_birth_day               integer                       ,\n",
    "    c_birth_month             integer                       ,\n",
    "    c_birth_year              integer                       ,\n",
    "    c_birth_country           varchar(20)                   ,\n",
    "    c_login                   char(13)                      ,\n",
    "    c_email_address           char(50)                      ,\n",
    "    c_last_review_date_sk     integer                       \n",
    "\"\"\",\n",
    "\"customer_address\" -> \"\"\"\n",
    "    ca_address_sk             integer               not null,\n",
    "    ca_address_id             char(16)              not null,\n",
    "    ca_street_number          char(10)                      ,\n",
    "    ca_street_name            varchar(60)                   ,\n",
    "    ca_street_type            char(15)                      ,\n",
    "    ca_suite_number           char(10)                      ,\n",
    "    ca_city                   varchar(60)                   ,\n",
    "    ca_county                 varchar(30)                   ,\n",
    "    ca_state                  char(2)                       ,\n",
    "    ca_zip                    char(10)                      ,\n",
    "    ca_country                varchar(20)                   ,\n",
    "    ca_gmt_offset             decimal(5,2)                  ,\n",
    "    ca_location_type          char(20)                      \n",
    "\"\"\",\n",
    "\"customer_demographics\" -> \"\"\"\n",
    "    cd_demo_sk                integer               not null,\n",
    "    cd_gender                 char(1)                       ,\n",
    "    cd_marital_status         char(1)                       ,\n",
    "    cd_education_status       char(20)                      ,\n",
    "    cd_purchase_estimate      integer                       ,\n",
    "    cd_credit_rating          char(10)                      ,\n",
    "    cd_dep_count              integer                       ,\n",
    "    cd_dep_employed_count     integer                       ,\n",
    "    cd_dep_college_count      integer                       \n",
    "\"\"\",\n",
    "\"date_dim\" -> \"\"\"\n",
    "    d_date_sk                 integer               not null,\n",
    "    d_date_id                 char(16)              not null,\n",
    "    d_date                    date                          ,\n",
    "    d_month_seq               integer                       ,\n",
    "    d_week_seq                integer                       ,\n",
    "    d_quarter_seq             integer                       ,\n",
    "    d_year                    integer                       ,\n",
    "    d_dow                     integer                       ,\n",
    "    d_moy                     integer                       ,\n",
    "    d_dom                     integer                       ,\n",
    "    d_qoy                     integer                       ,\n",
    "    d_fy_year                 integer                       ,\n",
    "    d_fy_quarter_seq          integer                       ,\n",
    "    d_fy_week_seq             integer                       ,\n",
    "    d_day_name                char(9)                       ,\n",
    "    d_quarter_name            char(6)                       ,\n",
    "    d_holiday                 char(1)                       ,\n",
    "    d_weekend                 char(1)                       ,\n",
    "    d_following_holiday       char(1)                       ,\n",
    "    d_first_dom               integer                       ,\n",
    "    d_last_dom                integer                       ,\n",
    "    d_same_day_ly             integer                       ,\n",
    "    d_same_day_lq             integer                       ,\n",
    "    d_current_day             char(1)                       ,\n",
    "    d_current_week            char(1)                       ,\n",
    "    d_current_month           char(1)                       ,\n",
    "    d_current_quarter         char(1)                       ,\n",
    "    d_current_year            char(1)                       \n",
    "\"\"\",\n",
    "\"household_demographics\" -> \"\"\"\n",
    "    hd_demo_sk                integer               not null,\n",
    "    hd_income_band_sk         integer                       ,\n",
    "    hd_buy_potential          char(15)                      ,\n",
    "    hd_dep_count              integer                       ,\n",
    "    hd_vehicle_count          integer                       \n",
    "\"\"\",\n",
    "\n",
    "\"income_band\" -> \"\"\"\n",
    "    ib_income_band_sk         integer               not null,\n",
    "    ib_lower_bound            integer                       ,\n",
    "    ib_upper_bound            integer                       \n",
    "\"\"\",\n",
    "\"inventory\" -> \"\"\"\n",
    "    inv_date_sk               integer               not null,\n",
    "    inv_item_sk               integer               not null,\n",
    "    inv_warehouse_sk          integer               not null,\n",
    "    inv_quantity_on_hand      integer                       \n",
    "\"\"\",\n",
    "\"item\" -> \"\"\"\n",
    "    i_item_sk                 integer               not null,\n",
    "    i_item_id                 char(16)              not null,\n",
    "    i_rec_start_date          date                          ,\n",
    "    i_rec_end_date            date                          ,\n",
    "    i_item_desc               varchar(200)                  ,\n",
    "    i_current_price           decimal(7,2)                  ,\n",
    "    i_wholesale_cost          decimal(7,2)                  ,\n",
    "    i_brand_id                integer                       ,\n",
    "    i_brand                   char(50)                      ,\n",
    "    i_class_id                integer                       ,\n",
    "    i_class                   char(50)                      ,\n",
    "    i_category_id             integer                       ,\n",
    "    i_category                char(50)                      ,\n",
    "    i_manufact_id             integer                       ,\n",
    "    i_manufact                char(50)                      ,\n",
    "    i_size                    char(20)                      ,\n",
    "    i_formulation             char(20)                      ,\n",
    "    i_color                   char(20)                      ,\n",
    "    i_units                   char(10)                      ,\n",
    "    i_container               char(10)                      ,\n",
    "    i_manager_id              integer                       ,\n",
    "    i_product_name            char(50)                      \n",
    "\"\"\",\n",
    "\"promotion\" -> \"\"\"\n",
    "    p_promo_sk                integer               not null,\n",
    "    p_promo_id                char(16)              not null,\n",
    "    p_start_date_sk           integer                       ,\n",
    "    p_end_date_sk             integer                       ,\n",
    "    p_item_sk                 integer                       ,\n",
    "    p_cost                    decimal(15,2)                 ,\n",
    "    p_response_target         integer                       ,\n",
    "    p_promo_name              char(50)                      ,\n",
    "    p_channel_dmail           char(1)                       ,\n",
    "    p_channel_email           char(1)                       ,\n",
    "    p_channel_catalog         char(1)                       ,\n",
    "    p_channel_tv              char(1)                       ,\n",
    "    p_channel_radio           char(1)                       ,\n",
    "    p_channel_press           char(1)                       ,\n",
    "    p_channel_event           char(1)                       ,\n",
    "    p_channel_demo            char(1)                       ,\n",
    "    p_channel_details         varchar(100)                  ,\n",
    "    p_purpose                 char(15)                      ,\n",
    "    p_discount_active         char(1)                       \n",
    "\"\"\",\n",
    "\"reason\" -> \"\"\"\n",
    "    r_reason_sk               integer               not null,\n",
    "    r_reason_id               char(16)              not null,\n",
    "    r_reason_desc             char(100)                     \n",
    "\"\"\",\n",
    "\"ship_mode\" -> \"\"\"\n",
    "    sm_ship_mode_sk           integer               not null,\n",
    "    sm_ship_mode_id           char(16)              not null,\n",
    "    sm_type                   char(30)                      ,\n",
    "    sm_code                   char(10)                      ,\n",
    "    sm_carrier                char(20)                      ,\n",
    "    sm_contract               char(20)                      \n",
    "\"\"\",\n",
    "\"store\" -> \"\"\"\n",
    "    s_store_sk                integer               not null,\n",
    "    s_store_id                char(16)              not null,\n",
    "    s_rec_start_date          date                          ,\n",
    "    s_rec_end_date            date                          ,\n",
    "    s_closed_date_sk          integer                       ,\n",
    "    s_store_name              varchar(50)                   ,\n",
    "    s_number_employees        integer                       ,\n",
    "    s_floor_space             integer                       ,\n",
    "    s_hours                   char(20)                      ,\n",
    "    s_manager                 varchar(40)                   ,\n",
    "    s_market_id               integer                       ,\n",
    "    s_geography_class         varchar(100)                  ,\n",
    "    s_market_desc             varchar(100)                  ,\n",
    "    s_market_manager          varchar(40)                   ,\n",
    "    s_division_id             integer                       ,\n",
    "    s_division_name           varchar(50)                   ,\n",
    "    s_company_id              integer                       ,\n",
    "    s_company_name            varchar(50)                   ,\n",
    "    s_street_number           varchar(10)                   ,\n",
    "    s_street_name             varchar(60)                   ,\n",
    "    s_street_type             char(15)                      ,\n",
    "    s_suite_number            char(10)                      ,\n",
    "    s_city                    varchar(60)                   ,\n",
    "    s_county                  varchar(30)                   ,\n",
    "    s_state                   char(2)                       ,\n",
    "    s_zip                     char(10)                      ,\n",
    "    s_country                 varchar(20)                   ,\n",
    "    s_gmt_offset              decimal(5,2)                  ,\n",
    "    s_tax_precentage          decimal(5,2)                  \n",
    "\"\"\",\n",
    "\"store_returns\" -> \"\"\"\n",
    "    sr_returned_date_sk       integer                       ,\n",
    "    sr_return_time_sk         integer                       ,\n",
    "    sr_item_sk                integer               not null,\n",
    "    sr_customer_sk            integer                       ,\n",
    "    sr_cdemo_sk               integer                       ,\n",
    "    sr_hdemo_sk               integer                       ,\n",
    "    sr_addr_sk                integer                       ,\n",
    "    sr_store_sk               integer                       ,\n",
    "    sr_reason_sk              integer                       ,\n",
    "    sr_ticket_number          bigint                not null,\n",
    "    sr_return_quantity        integer                       ,\n",
    "    sr_return_amt             decimal(7,2)                  ,\n",
    "    sr_return_tax             decimal(7,2)                  ,\n",
    "    sr_return_amt_inc_tax     decimal(7,2)                  ,\n",
    "    sr_fee                    decimal(7,2)                  ,\n",
    "    sr_return_ship_cost       decimal(7,2)                  ,\n",
    "    sr_refunded_cash          decimal(7,2)                  ,\n",
    "    sr_reversed_charge        decimal(7,2)                  ,\n",
    "    sr_store_credit           decimal(7,2)                  ,\n",
    "    sr_net_loss               decimal(7,2)                  \n",
    "\"\"\",\n",
    "\n",
    "\"store_sales\" -> \"\"\"\n",
    "    ss_sold_date_sk           integer                       ,\n",
    "    ss_sold_time_sk           integer                       ,\n",
    "    ss_item_sk                integer               not null,\n",
    "    ss_customer_sk            integer                       ,\n",
    "    ss_cdemo_sk               integer                       ,\n",
    "    ss_hdemo_sk               integer                       ,\n",
    "    ss_addr_sk                integer                       ,\n",
    "    ss_store_sk               integer                       ,\n",
    "    ss_promo_sk               integer                       ,\n",
    "    ss_ticket_number          bigint                not null,\n",
    "    ss_quantity               integer                       ,\n",
    "    ss_wholesale_cost         decimal(7,2)                  ,\n",
    "    ss_list_price             decimal(7,2)                  ,\n",
    "    ss_sales_price            decimal(7,2)                  ,\n",
    "    ss_ext_discount_amt       decimal(7,2)                  ,\n",
    "    ss_ext_sales_price        decimal(7,2)                  ,\n",
    "    ss_ext_wholesale_cost     decimal(7,2)                  ,\n",
    "    ss_ext_list_price         decimal(7,2)                  ,\n",
    "    ss_ext_tax                decimal(7,2)                  ,\n",
    "    ss_coupon_amt             decimal(7,2)                  ,\n",
    "    ss_net_paid               decimal(7,2)                  ,\n",
    "    ss_net_paid_inc_tax       decimal(7,2)                  ,\n",
    "    ss_net_profit             decimal(7,2)                  \n",
    "\"\"\",\n",
    "\"time_dim\" -> \"\"\"\n",
    "    t_time_sk                 integer               not null,\n",
    "    t_time_id                 char(16)              not null,\n",
    "    t_time                    integer                       ,\n",
    "    t_hour                    integer                       ,\n",
    "    t_minute                  integer                       ,\n",
    "    t_second                  integer                       ,\n",
    "    t_am_pm                   char(2)                       ,\n",
    "    t_shift                   char(20)                      ,\n",
    "    t_sub_shift               char(20)                      ,\n",
    "    t_meal_time               char(20)                      \n",
    "\"\"\",\n",
    "\"warehouse\" -> \"\"\"\n",
    "    w_warehouse_sk            integer               not null,\n",
    "    w_warehouse_id            char(16)              not null,\n",
    "    w_warehouse_name          varchar(20)                   ,\n",
    "    w_warehouse_sq_ft         integer                       ,\n",
    "    w_street_number           char(10)                      ,\n",
    "    w_street_name             varchar(60)                   ,\n",
    "    w_street_type             char(15)                      ,\n",
    "    w_suite_number            char(10)                      ,\n",
    "    w_city                    varchar(60)                   ,\n",
    "    w_county                  varchar(30)                   ,\n",
    "    w_state                   char(2)                       ,\n",
    "    w_zip                     char(10)                      ,\n",
    "    w_country                 varchar(20)                   ,\n",
    "    w_gmt_offset              decimal(5,2)                  \n",
    "\"\"\",\n",
    "\"web_page\" -> \"\"\"\n",
    "    wp_web_page_sk            integer               not null,\n",
    "    wp_web_page_id            char(16)              not null,\n",
    "    wp_rec_start_date         date                          ,\n",
    "    wp_rec_end_date           date                          ,\n",
    "    wp_creation_date_sk       integer                       ,\n",
    "    wp_access_date_sk         integer                       ,\n",
    "    wp_autogen_flag           char(1)                       ,\n",
    "    wp_customer_sk            integer                       ,\n",
    "    wp_url                    varchar(100)                  ,\n",
    "    wp_type                   char(50)                      ,\n",
    "    wp_char_count             integer                       ,\n",
    "    wp_link_count             integer                       ,\n",
    "    wp_image_count            integer                       ,\n",
    "    wp_max_ad_count           integer                       \n",
    "\"\"\",\n",
    "\"web_returns\" -> \"\"\"\n",
    "    wr_returned_date_sk       integer                       ,\n",
    "    wr_returned_time_sk       integer                       ,\n",
    "    wr_item_sk                integer               not null,\n",
    "    wr_refunded_customer_sk   integer                       ,\n",
    "    wr_refunded_cdemo_sk      integer                       ,\n",
    "    wr_refunded_hdemo_sk      integer                       ,\n",
    "    wr_refunded_addr_sk       integer                       ,\n",
    "    wr_returning_customer_sk  integer                       ,\n",
    "    wr_returning_cdemo_sk     integer                       ,\n",
    "    wr_returning_hdemo_sk     integer                       ,\n",
    "    wr_returning_addr_sk      integer                       ,\n",
    "    wr_web_page_sk            integer                       ,\n",
    "    wr_reason_sk              integer                       ,\n",
    "    wr_order_number           bigint                not null,\n",
    "    wr_return_quantity        integer                       ,\n",
    "    wr_return_amt             decimal(7,2)                  ,\n",
    "    wr_return_tax             decimal(7,2)                  ,\n",
    "    wr_return_amt_inc_tax     decimal(7,2)                  ,\n",
    "    wr_fee                    decimal(7,2)                  ,\n",
    "    wr_return_ship_cost       decimal(7,2)                  ,\n",
    "    wr_refunded_cash          decimal(7,2)                  ,\n",
    "    wr_reversed_charge        decimal(7,2)                  ,\n",
    "    wr_account_credit         decimal(7,2)                  ,\n",
    "    wr_net_loss               decimal(7,2)                  \n",
    "\"\"\",\n",
    "\"web_sales\" -> \"\"\"\n",
    "    ws_sold_date_sk           integer                       ,\n",
    "    ws_sold_time_sk           integer                       ,\n",
    "    ws_ship_date_sk           integer                       ,\n",
    "    ws_item_sk                integer               not null,\n",
    "    ws_bill_customer_sk       integer                       ,\n",
    "    ws_bill_cdemo_sk          integer                       ,\n",
    "    ws_bill_hdemo_sk          integer                       ,\n",
    "    ws_bill_addr_sk           integer                       ,\n",
    "    ws_ship_customer_sk       integer                       ,\n",
    "    ws_ship_cdemo_sk          integer                       ,\n",
    "    ws_ship_hdemo_sk          integer                       ,\n",
    "    ws_ship_addr_sk           integer                       ,\n",
    "    ws_web_page_sk            integer                       ,\n",
    "    ws_web_site_sk            integer                       ,\n",
    "    ws_ship_mode_sk           integer                       ,\n",
    "    ws_warehouse_sk           integer                       ,\n",
    "    ws_promo_sk               integer                       ,\n",
    "    ws_order_number           bigint                not null,\n",
    "    ws_quantity               integer                       ,\n",
    "    ws_wholesale_cost         decimal(7,2)                  ,\n",
    "    ws_list_price             decimal(7,2)                  ,\n",
    "    ws_sales_price            decimal(7,2)                  ,\n",
    "    ws_ext_discount_amt       decimal(7,2)                  ,\n",
    "    ws_ext_sales_price        decimal(7,2)                  ,\n",
    "    ws_ext_wholesale_cost     decimal(7,2)                  ,\n",
    "    ws_ext_list_price         decimal(7,2)                  ,\n",
    "    ws_ext_tax                decimal(7,2)                  ,\n",
    "    ws_coupon_amt             decimal(7,2)                  ,\n",
    "    ws_ext_ship_cost          decimal(7,2)                  ,\n",
    "    ws_net_paid               decimal(7,2)                  ,\n",
    "    ws_net_paid_inc_tax       decimal(7,2)                  ,\n",
    "    ws_net_paid_inc_ship      decimal(7,2)                  ,\n",
    "    ws_net_paid_inc_ship_tax  decimal(7,2)                  ,\n",
    "    ws_net_profit             decimal(7,2)                  \n",
    "\"\"\",\n",
    "\"web_site\" -> \"\"\"\n",
    "    web_site_sk               integer               not null,\n",
    "    web_site_id               char(16)              not null,\n",
    "    web_rec_start_date        date                          ,\n",
    "    web_rec_end_date          date                          ,\n",
    "    web_name                  varchar(50)                   ,\n",
    "    web_open_date_sk          integer                       ,\n",
    "    web_close_date_sk         integer                       ,\n",
    "    web_class                 varchar(50)                   ,\n",
    "    web_manager               varchar(40)                   ,\n",
    "    web_mkt_id                integer                       ,\n",
    "    web_mkt_class             varchar(50)                   ,\n",
    "    web_mkt_desc              varchar(100)                  ,\n",
    "    web_market_manager        varchar(40)                   ,\n",
    "    web_company_id            integer                       ,\n",
    "    web_company_name          char(50)                      ,\n",
    "    web_street_number         char(10)                      ,\n",
    "    web_street_name           varchar(60)                   ,\n",
    "    web_street_type           char(15)                      ,\n",
    "    web_suite_number          char(10)                      ,\n",
    "    web_city                  varchar(60)                   ,\n",
    "    web_county                varchar(30)                   ,\n",
    "    web_state                 char(2)                       ,\n",
    "    web_zip                   char(10)                      ,\n",
    "    web_country               varchar(20)                   ,\n",
    "    web_gmt_offset            decimal(5,2)                  ,\n",
    "    web_tax_percentage        decimal(5,2)                  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "//TPC-H\n",
    "val tpchTableColumnSchemas = Map(\n",
    "\"customer\" -> \"\"\"\n",
    "        c_custkey BIGINT,\n",
    "        c_name VARCHAR(25),\n",
    "        c_address VARCHAR(40),\n",
    "        c_nationkey BIGINT,\n",
    "        c_phone CHAR(15),\n",
    "        c_acctbal DECIMAL(18,2),\n",
    "        c_mktsegment CHAR(10),\n",
    "        c_comment VARCHAR(117)\n",
    "\"\"\",\n",
    "\"lineitem\" -> \"\"\"\n",
    "        l_orderkey BIGINT,\n",
    "        l_partkey BIGINT,\n",
    "        l_suppkey BIGINT,\n",
    "        l_linenumber INTEGER,\n",
    "        l_quantity DECIMAL(18,2),\n",
    "        l_extendedprice DECIMAL(18,2),\n",
    "        l_discount DECIMAL(18,2),\n",
    "        l_tax DECIMAL(18,2),\n",
    "        l_returnflag CHAR(1),\n",
    "        l_linestatus CHAR(1),\n",
    "        l_shipdate DATE,\n",
    "        l_commitdate DATE,\n",
    "        l_receiptdate DATE,\n",
    "        l_shipinstruct CHAR(25),\n",
    "        l_shipmode CHAR(10),\n",
    "        l_comment VARCHAR(44)\n",
    "\"\"\",\n",
    "\"nation\" -> \"\"\"\n",
    "        n_nationkey BIGINT,\n",
    "        n_name CHAR(25),\n",
    "        n_regionkey BIGINT,\n",
    "        n_comment VARCHAR(152)\n",
    "\"\"\",\n",
    "\"orders\" -> \"\"\"\n",
    "        o_orderkey BIGINT,\n",
    "        o_custkey BIGINT,\n",
    "        o_orderstatus CHAR(1),\n",
    "        o_totalprice DECIMAL(18,2),\n",
    "        o_orderdate DATE,\n",
    "        o_orderpriority CHAR(15),\n",
    "        o_clerk CHAR(15),\n",
    "        o_shippriority INTEGER,\n",
    "        o_comment VARCHAR(79)\n",
    "\"\"\",\n",
    "\"part\" -> \"\"\"\n",
    "        p_partkey BIGINT,\n",
    "        p_name VARCHAR(55),\n",
    "        p_mfgr CHAR(25),\n",
    "        p_brand CHAR(10),\n",
    "        p_type VARCHAR(25),\n",
    "        p_size INTEGER,\n",
    "        p_container CHAR(10),\n",
    "        p_retailprice DECIMAL(18,2),\n",
    "        p_comment VARCHAR(23)\n",
    "\"\"\",\n",
    "\"partsupp\" -> \"\"\"\n",
    "        ps_partkey BIGINT,\n",
    "        ps_suppkey BIGINT,\n",
    "        ps_availqty INTEGER,\n",
    "        ps_supplycost DECIMAL(18,2),\n",
    "        ps_comment VARCHAR(199)\n",
    "\"\"\",\n",
    "\"region\" -> \"\"\"\n",
    "        r_regionkey BIGINT,\n",
    "        r_name CHAR(25),\n",
    "        r_comment VARCHAR(152)\n",
    "\"\"\",\n",
    "\"supplier\" -> \"\"\"\n",
    "        s_suppkey BIGINT,\n",
    "        s_name CHAR(25),\n",
    "        s_address VARCHAR(40),\n",
    "        s_nationkey BIGINT,\n",
    "        s_phone CHAR(15),\n",
    "        s_acctbal DECIMAL(18,2),\n",
    "        s_comment VARCHAR(101)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def getBenchmarkColumns(benchmark: String) = benchmark match {\n",
    "  case \"TPCDS\" => tableColumnSchemas\n",
    "  case \"TPCH\" => tpchTableColumnSchemas\n",
    "  case _ => throw new Exception(s\"Invalid benchmark $benchmark\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd3dd4a-75df-4b91-bd64-f9b40236c2bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Clustering keys for TPC- DS and H"
    }
   },
   "outputs": [],
   "source": [
    "val tableClusteringKeys = Map(\n",
    "  \"dbgen_version\"         -> Seq(\"dv_create_date\"),\n",
    "  \"call_center\"           -> Seq(\"cc_call_center_sk\"),\n",
    "  \"catalog_page\"          -> Seq(\"cp_catalog_page_sk\"),\n",
    "  \"catalog_returns\"       -> Seq(\"cr_returned_date_sk\", \"cr_item_sk\"),\n",
    "  \"catalog_sales\"         -> Seq(\"cs_sold_date_sk\", \"cs_item_sk\"),\n",
    "  \"customer\"              -> Seq(\"c_customer_sk\"),\n",
    "  \"customer_address\"      -> Seq(\"ca_address_sk\"),\n",
    "  \"customer_demographics\" -> Seq(\"cd_demo_sk\"),\n",
    "  \"date_dim\"              -> Seq(\"d_date_sk\"),\n",
    "  \"household_demographics\"-> Seq(\"hd_demo_sk\"),\n",
    "  \"income_band\"           -> Seq(\"ib_income_band_sk\"),\n",
    "  \"inventory\"             -> Seq(\"inv_date_sk\", \"inv_item_sk\"),\n",
    "  \"item\"                  -> Seq(\"i_item_sk\"),\n",
    "  \"promotion\"             -> Seq(\"p_promo_sk\"),\n",
    "  \"reason\"                -> Seq(\"r_reason_sk\"),\n",
    "  \"ship_mode\"             -> Seq(\"sm_ship_mode_sk\"),\n",
    "  \"store\"                 -> Seq(\"s_store_sk\"),\n",
    "  \"store_returns\"         -> Seq(\"sr_returned_date_sk\", \"sr_item_sk\"),\n",
    "  \"store_sales\"           -> Seq(\"ss_sold_date_sk\", \"ss_item_sk\"),\n",
    "  \"time_dim\"              -> Seq(\"t_time_sk\"),\n",
    "  \"warehouse\"             -> Seq(\"w_warehouse_sk\"),\n",
    "  \"web_page\"              -> Seq(\"wp_web_page_sk\"),\n",
    "  \"web_returns\"           -> Seq(\"wr_returned_date_sk\", \"wr_item_sk\"),\n",
    "  \"web_sales\"             -> Seq(\"ws_sold_date_sk\", \"ws_item_sk\"),\n",
    "  \"web_site\"              -> Seq(\"web_site_sk\")\n",
    ")\n",
    "\n",
    "val tpchTableClusteringKeys = Map(\n",
    "  \"customer\" -> Seq(\"c_mktsegment\"),  \n",
    "  \"lineitem\" -> Seq(\"l_shipdate\"),\n",
    "  \"nation\" -> Seq(\"\"),\n",
    "  \"orders\" -> Seq(\"o_orderdate\"),\n",
    "  \"part\" -> Seq(\"\"),\n",
    "  \"partsupp\" -> Seq(\"\"),\n",
    "  \"region\" -> Seq(\"\"),\n",
    "  \"supplier\" -> Seq(\"\")\n",
    ")\n",
    "\n",
    "def getBenchmarkClustering(benchmark: String) = benchmark match {\n",
    "  case \"TPCDS\" => tableClusteringKeys\n",
    "  case \"TPCH\" => tpchTableClusteringKeys\n",
    "  case _ => throw new Exception(s\"Invalid benchmark $benchmark\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73237792-5c9a-467a-b8e8-ce99166d5c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import java.util.concurrent.LinkedBlockingQueue\n",
    "import scala.collection.immutable.Stream\n",
    "import scala.sys.process._\n",
    "\n",
    "/**\n",
    " * Using ProcessBuilder.lineStream produces a stream, that uses\n",
    " * a LinkedBlockingQueue with a default capacity of Integer.MAX_VALUE.\n",
    " *\n",
    " * This causes OOM if the consumer cannot keep up with the producer.\n",
    " *\n",
    " * See scala.sys.process.ProcessBuilderImpl.lineStream\n",
    " */\n",
    "object BlockingLineStream {\n",
    "  // See scala.sys.process.Streamed\n",
    "  private final class BlockingStreamed[T](\n",
    "    val process:   T => Unit,\n",
    "    val    done: Int => Unit,\n",
    "    val  stream:  () => Stream[T]\n",
    "  )\n",
    "\n",
    "  // See scala.sys.process.Streamed\n",
    "  private object BlockingStreamed {\n",
    "    // scala.process.sys.Streamed uses default of Integer.MAX_VALUE,\n",
    "    // which causes OOMs if the consumer cannot keep up with producer.\n",
    "    val maxQueueSize = 65536\n",
    "\n",
    "    def apply[T](nonzeroException: Boolean): BlockingStreamed[T] = {\n",
    "      val q = new LinkedBlockingQueue[Either[Int, T]](maxQueueSize)\n",
    "\n",
    "      def next(): Stream[T] = q.take match {\n",
    "        case Left(0) => Stream.empty\n",
    "        case Left(code) =>\n",
    "          if (nonzeroException) scala.sys.error(\"Nonzero exit code: \" + code) else Stream.empty\n",
    "        case Right(s) => Stream.cons(s, next())\n",
    "      }\n",
    "\n",
    "      new BlockingStreamed((s: T) => q put Right(s), code => q put Left(code), () => next())\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // See scala.sys.process.ProcessImpl.Spawn\n",
    "  private object Spawn {\n",
    "    def apply(f: => Unit): Thread = apply(f, daemon = false)\n",
    "    def apply(f: => Unit, daemon: Boolean): Thread = {\n",
    "      val thread = new Thread() { override def run() = { f } }\n",
    "      thread.setDaemon(daemon)\n",
    "      thread.start()\n",
    "      thread\n",
    "    }\n",
    "  }\n",
    "\n",
    "  def apply(command: Seq[String]): Stream[String] = {\n",
    "    val streamed = BlockingStreamed[String](true)\n",
    "    val process = command.run(BasicIO(false, streamed.process, None))\n",
    "    Spawn(streamed.done(process.exitValue()))\n",
    "    streamed.stream()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0640495d-3dc3-46fc-b5cd-359593b69ea3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Settings for datagens and database creation"
    }
   },
   "outputs": [],
   "source": [
    "val partitions = workers * cores * 2\n",
    "val dsdgen = s\"${baseDatagenFolder}/dsdgen/dsdgen\"\n",
    "val dbgen = s\"${baseDatagenFolder}/dbgen/dbgen\"\n",
    "\n",
    "//val convertToRows = false\n",
    "//val convertToSchema = false\n",
    "\n",
    "def createDatabase(databaseName: String) = {\n",
    "  log(s\"Creating schema $databaseName\")            \n",
    "  if(overwrite) time(s\"drop-cascade_$databaseName\", sql(s\"DROP DATABASE IF EXISTS $databaseName CASCADE\"))\n",
    "  sql(s\"CREATE DATABASE IF NOT EXISTS $databaseName\")\n",
    "  sql(s\"USE $databaseName\")            \n",
    "}\n",
    "\n",
    "def checkBin(fp: String) = {\n",
    "  if (new java.io.File(fp).exists) {\n",
    "    true\n",
    "  } else {\n",
    "    sys.error(s\"Could not find the file at $fp. Check the tool builder above\")\n",
    "  }  \n",
    "}\n",
    "\n",
    "def tpcdsCmd(tableName: String, scaleFactor: String, part: Int) = {\n",
    "  checkBin(dsdgen)\n",
    "  val localToolsDir = s\"$baseDatagenFolder/dsdgen\"\n",
    "  // Note: RNGSEED is the RNG seed used by the data generator. Right now, it is fixed to 100.\n",
    "  val parallel = if (partitions > 1) s\"-parallel $partitions -child $part\" else \"\"\n",
    "  val commands = Seq(\n",
    "    \"bash\", \"-c\",\n",
    "    s\"cd $localToolsDir && ./dsdgen -table $tableName -filter Y -scale $scaleFactor -RNGSEED 100 $parallel\")  \n",
    "  commands\n",
    "}\n",
    "\n",
    "val smallTpchTables = Seq(\"nation\", \"region\")\n",
    "\n",
    "def tpchCmd(tableName: String, scaleFactor: String, part: Int) = {\n",
    "  checkBin(dbgen)\n",
    "  val localToolsDir = s\"$baseDatagenFolder/dbgen\"\n",
    "  val shortTableNames = Map(\n",
    "    \"customer\" -> \"c\",\n",
    "    \"lineitem\" -> \"L\",\n",
    "    \"nation\" -> \"n\",\n",
    "    \"orders\" -> \"O\",\n",
    "    \"part\" -> \"P\",\n",
    "    \"region\" -> \"r\",\n",
    "    \"supplier\" -> \"s\",\n",
    "    \"partsupp\" -> \"S\"\n",
    "  )\n",
    "  val parallel = if (partitions > 1 && !smallTpchTables.contains(tableName)) s\"-C $partitions -S $part\" else \"\"\n",
    "  val commands = Seq(\n",
    "    \"bash\", \"-c\",\n",
    "    s\"cd $localToolsDir && ./dbgen -T ${shortTableNames(tableName)} -s $scaleFactor $parallel\")\n",
    "  commands\n",
    "}\n",
    "\n",
    "def tpcCmd(benchmark: String, tableName: String, scaleFactor: String, part: Int) = benchmark match {\n",
    "  case \"TPCDS\" => tpcdsCmd(tableName, scaleFactor, part)\n",
    "  case \"TPCH\" => tpchCmd(tableName, scaleFactor, part)\n",
    "  case _ => throw new Exception(s\"Benchmark $benchmark not supported.\")  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7fe3e9-2e54-4e31-8142-f411cc0857fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "File writers"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "// Duplicate TPCDS column to be compatible with older queries (pre v2.10)\n",
    "def addExtraColumn(df: DataFrame, benchmark: String, tableName: String) = {\n",
    "  if (benchmark == \"TPCDS\" && tableName == \"customer\")\n",
    "    df.withColumn(\"c_last_review_date\", $\"c_last_review_date_sk\")\n",
    "  else\n",
    "    df\n",
    "}\n",
    "\n",
    "def writeFormat(ds: Dataset[String], benchmark: String, tableName: String) = {\n",
    "  val csvTable = spark.read\n",
    "    .option(\"delimiter\", \"|\")\n",
    "    .option(\"sep\", \"|\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"emptyValue\", \"\")\n",
    "    .option(\"charset\", \"iso-8859-1\")\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss[.SSS]\") // -- spec: yyyy-mm-dd hh:mm:ss.s\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"multiLine\", \"false\")\n",
    "    .option(\"locale\", \"en-US\")\n",
    "    .option(\"lineSep\", \"\\n\")        \n",
    "    .schema(getBenchmarkColumns(benchmark)(tableName) + \", last_col string\")\n",
    "    .csv(ds)\n",
    "    .drop(\"last_col\")\n",
    "\n",
    "  val csvTableOut = addExtraColumn(csvTable, benchmark, tableName)\n",
    "  \n",
    "  if (!clusterTables || getBenchmarkClustering(benchmark)(tableName)(0).isEmpty) {\n",
    "    // Non-Liquid delta tables\n",
    "    if (Seq(\"delta\", \"tahoe\").contains(fileFormat)) {\n",
    "    csvTableOut.write\n",
    "      .format(fileFormat)\n",
    "      .mode(if (overwrite) \"overwrite\" else \"ignore\")\n",
    "      .option(\"overwriteSchema\", \"true\")      \n",
    "      .saveAsTable(tableName)      \n",
    "    } else {\n",
    "      // For other formats, coalesce to produce fewer files\n",
    "      csvTableOut\n",
    "        .coalesce(coalesceInto)\n",
    "        .write\n",
    "        .format(fileFormat)\n",
    "        .mode(if (overwrite) \"overwrite\" else \"ignore\")\n",
    "        .option(\"overwriteSchema\", \"true\")      \n",
    "        .saveAsTable(tableName)       \n",
    "    }\n",
    "  } else if (distributeStrategy == \"none\") {\n",
    "    // Delta tables uses the optimized writer  \n",
    "    val clusteringCols = getBenchmarkClustering(benchmark)(tableName)\n",
    "    if (clusteringCols.nonEmpty) {\n",
    "       csvTableOut.write\n",
    "      .format(fileFormat)\n",
    "      .mode(if (overwrite) \"overwrite\" else \"ignore\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .clusterBy(clusteringCols.head, clusteringCols.tail: _*)\n",
    "      .saveAsTable(tableName)\n",
    "    } else {\n",
    "  // fallback if there are no clustering columns\n",
    "    csvTableOut.write\n",
    "    .format(fileFormat)\n",
    "    .mode(if (overwrite) \"overwrite\" else \"ignore\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(tableName)\n",
    "}\n",
    "  } else {\n",
    "    // Parquet, orc, etc need a repartition\n",
    "    csvTableOut\n",
    "      .repartition(col(getBenchmarkClustering(benchmark)(tableName).head))\n",
    "      .write\n",
    "      .format(fileFormat)\n",
    "      .mode(if (overwrite) \"overwrite\" else \"ignore\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .partitionBy(getBenchmarkClustering(benchmark)(tableName):_*)      \n",
    "      .saveAsTable(tableName)\n",
    "  }\n",
    "}\n",
    "\n",
    "def createTable(benchmark: String, tableName: String) = {\n",
    "  val partitionedBy = \n",
    "    if (!clusterTables || getBenchmarkClustering(benchmark)(tableName)(0).isEmpty) \"\" \n",
    "    else \"CLUSTER BY \" + getBenchmarkClustering(benchmark)(tableName).mkString(\", \")\n",
    "  // (${getBenchmarkColumns(benchmark)(tableName)})  \n",
    "  val ct = s\"\"\"CREATE TABLE IF NOT EXISTS $tableName USING $fileFormat \"\"\"\n",
    "  println(ct)\n",
    "  sql(ct)\n",
    "  // recover the partitions if not delta and partitioned\n",
    "  if (!Seq(\"delta\", \"tahoe\").contains(fileFormat) && clusterTables && !getBenchmarkClustering(benchmark)(tableName)(0).isEmpty)\n",
    "    sql(s\"MSCK REPAIR TABLE $tableName\")\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "// Retry-once function to recover from file exists errros (ie., on don't overwirte)\n",
    "// AnalysisException: Cannot create table t.\n",
    "def writeWithReties[R](fn: => R, retries: Int = 1, benchmark: String, tableName: String) = {\n",
    "  var tries = retries\n",
    "  while(tries >= 0) {\n",
    "    try{\n",
    "      fn\n",
    "      tries = -1\n",
    "    } catch {\n",
    "      case e: AnalysisException if tries > 1 && overwrite == false => {        \n",
    "        println(s\"Failed writing into table. Retries available $tries. Error: \" + e.getMessage)\n",
    "        tries = tries - 1\n",
    "        println(s\"Attmpting to drop and recreate table: $tableName\")\n",
    "        sql(s\"DROP TABLE IF EXISTS $tableName\")\n",
    "      }      \n",
    "      case e: Throwable if tries < 1 => throw e\n",
    "    }\n",
    "  }  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a79dc61-0813-4a49-b489-a3b08ed37517",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save text files using dataframes"
    }
   },
   "outputs": [],
   "source": [
    "import spark.implicits._ //needed for rdd to dataset conversion\n",
    "\n",
    "scaleFactors.foreach { scaleFactor => {\n",
    "  setScaleConfig(scaleFactor) // To prevent OOMs\n",
    "  benchmarks.foreach { benchmark => { \n",
    "    val outputFormat = fileFormat\n",
    "    time(s\"datagen_${benchmark}_${scaleFactor}_${outputFormat}\", {\n",
    "      val databaseName = getNameLocation(benchmark, scaleFactor)\n",
    "      createDatabase(databaseName)\n",
    "      for (tableName <- getBenchmarkTables(benchmark)) {        \n",
    "        time(s\"datagen_${benchmark}_${scaleFactor}_${outputFormat}_${tableName}\", {\n",
    "          val generatedData = {\n",
    "            val parts = if (benchmark == \"TPCH\" && smallTpchTables.contains(tableName)) 1 else partitions\n",
    "            sc.parallelize(1 to parts, parts).flatMap { part =>\n",
    "              val commands = tpcCmd(benchmark, tableName, scaleFactor, part)\n",
    "              println(commands)\n",
    "              BlockingLineStream(commands)\n",
    "            }\n",
    "          }\n",
    "          val jobName = s\"$benchmark $tableName sf=$scaleFactor\"\n",
    "          generatedData.setName(jobName)\n",
    "          sc.setJobGroup(jobName, \"\")\n",
    "          \n",
    "          writeWithReties(\n",
    "            writeFormat(generatedData.toDS, benchmark, tableName), \n",
    "            2, \n",
    "            benchmark, tableName\n",
    "          )\n",
    "      })\n",
    "    }\n",
    "  })\n",
    "}}\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ad81d1-11de-4e14-a7e5-36e45801cc2b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optimize delta tables"
    }
   },
   "outputs": [],
   "source": [
    "// Generate the data, import the tables, generate stats for selected benchmarks and scale factors\n",
    "if (!skipOptimize && Seq(\"delta\", \"tahoe\").contains(fileFormat)) {\n",
    "  // when overwritting, force re-optimize  \n",
    "  if (overwrite) spark.conf.set(\"spark.databricks.delta.optimize.zorder.mergeStrategy\", \"all\")\n",
    "  scaleFactors.foreach { scaleFactor => {  \n",
    "  // First set some config settings affecting OOMs/performance\n",
    "  setScaleConfig(scaleFactor)\n",
    "  \n",
    "  benchmarks.foreach{ benchmark => {\n",
    "    \n",
    "    val (dbname) = getBenchmarkData(benchmark, scaleFactor)\n",
    "    println(s\"\\nDB $dbname\")\n",
    "    sql(s\"use $dbname\")\n",
    "    println(s\"\\nOptimizing DB $dbname\")\n",
    "    if (benchmark == \"TPCDS\") {\n",
    "      if (clusterTables) {\n",
    "        val queries = Array(\n",
    "      \"optimize call_center\",\n",
    "      \"optimize catalog_page\",\n",
    "      \"optimize catalog_returns\",\n",
    "      \"optimize customer\",\n",
    "      \"optimize customer_address\",\n",
    "      \"optimize customer_demographics\",\n",
    "      \"optimize household_demographics\",\n",
    "      \"optimize income_band\",\n",
    "      \"optimize inventory\",\n",
    "      \"optimize item\",\n",
    "      \"optimize promotion\",\n",
    "      \"optimize reason\",\n",
    "      \"optimize store\",\n",
    "      \"optimize store_returns\",\n",
    "      \"optimize store_sales\",\n",
    "      \"optimize time_dim\",\n",
    "      \"optimize warehouse\",\n",
    "      \"optimize web_page\",\n",
    "      \"optimize web_returns\",\n",
    "      \"optimize web_sales\",\n",
    "      \"optimize catalog_sales\",\n",
    "      \"optimize web_site\",\n",
    "          ).foreach { query =>\n",
    "            println(s\"Running $query\")\n",
    "            time(s\"optimize_${benchmark}_${scaleFactor}_${fileFormat}_$query\", sql(query)) \n",
    "          }\n",
    "      } else {\n",
    "        val queries = Array(\n",
    "      \"optimize call_center\",\n",
    "      \"optimize catalog_page\",\n",
    "      \"optimize catalog_returns\",\n",
    "      \"optimize catalog_sales\",\n",
    "      \"optimize customer\",\n",
    "      \"optimize customer_address\",\n",
    "      \"optimize customer_demographics\",\n",
    "      \"optimize date_dim\",\n",
    "      \"optimize household_demographics\",\n",
    "      \"optimize income_band\",\n",
    "      \"optimize inventory\",\n",
    "      \"optimize item\",\n",
    "      \"optimize promotion\",\n",
    "      \"optimize reason\",\n",
    "      \"optimize store\",\n",
    "      \"optimize store_returns\",\n",
    "      \"optimize store_sales\",\n",
    "      \"optimize time_dim\",\n",
    "      \"optimize warehouse\",\n",
    "      \"optimize web_page\",\n",
    "      \"optimize web_returns\",\n",
    "      \"optimize web_sales\",\n",
    "      \"optimize web_site\",\n",
    "          ).foreach { query =>\n",
    "            println(s\"Running $query\")\n",
    "            time(s\"optimize_${benchmark}_${scaleFactor}_${fileFormat}_$query\", sql(query).show(false)) \n",
    "          }\n",
    "      }\n",
    "     } else {  // TPCH \n",
    "      time(s\"optimize_${benchmark}_${scaleFactor}_${fileFormat}\", {\n",
    "        sql(s\"show tables\").select(\"tableName\").collect().foreach{ tableName =>        \n",
    "          val name: String = tableName.toString().drop(1).dropRight(1)\n",
    "          println(s\"Optimizing table $name\")\n",
    "          scala.util.Try(sql(s\"optimize $name\").show(false))\n",
    "        } \n",
    "      })              \n",
    "     }\n",
    "    }}\n",
    "    }}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a454c6-222d-40dc-8501-9b7d74289248",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create table and column stats (if set)"
    }
   },
   "outputs": [],
   "source": [
    "// Generate the data, import the tables, generate stats for selected benchmarks and scale factors\n",
    "scaleFactors.foreach { scaleFactor => {\n",
    "  \n",
    "  // First set some config settings affecting OOMs/performance\n",
    "  setScaleConfig(scaleFactor)\n",
    "  \n",
    "  benchmarks.foreach{ benchmark => {\n",
    "    val dbname = getBenchmarkData(benchmark, scaleFactor)\n",
    "     println(s\"\\nDB $dbname\")\n",
    "\n",
    "    if (createTableStats) time(s\"create-stats_${benchmark}_${scaleFactor}_${fileFormat}\", {\n",
    "      getTables(dbname).map{ tableName => \n",
    "       println(s\"Creating table stats for $benchmark $scaleFactor table $tableName\")\n",
    "       time(s\"create-table-stats_${benchmark}_${scaleFactor}_${fileFormat}_$tableName\", sql(s\"ANALYZE TABLE $dbname.$tableName COMPUTE STATISTICS\"))\n",
    "       println(s\"Creating column stats for $benchmark $scaleFactor table $tableName\")\n",
    "       time(s\"create-column-stats_${benchmark}_${scaleFactor}_${fileFormat}_$tableName\", sql(s\"ANALYZE TABLE $dbname.$tableName COMPUTE STATISTICS FOR ALL COLUMNS\"))\n",
    "      }\n",
    "    })\n",
    "  }}\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d670827-ef02-4d6a-8a70-9e6680444378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val catalog = tryGetWidget(\"catalog\").getOrElse(\"main\")\n",
    "sql(s\"use catalog $catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a3872b-2cfe-46f7-8a8b-ebb773ee0fea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VACUUM"
    }
   },
   "outputs": [],
   "source": [
    "if (!skipChecks) {\n",
    "  spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\",\"false\")  \n",
    "  scaleFactors.foreach { scaleFactor =>\n",
    "    benchmarks.foreach{ benchmark => {\n",
    "      val dbname = getBenchmarkData(benchmark, scaleFactor)\n",
    "      sql(s\"use catalog $catalog\")\n",
    "      sql(s\"use $dbname\")\n",
    "      time(s\"vacuum_${benchmark}_${scaleFactor}_${fileFormat}\", {\n",
    "        sql(s\"show tables\").select(\"tableName\").collect().foreach{ tableName =>        \n",
    "          println(s\"Vacuuming $tableName\")\n",
    "          scala.util.Try(sql(s\"VACUUM `$tableName` RETAIN 0 HOURS\"))\n",
    "        } \n",
    "      })\n",
    "      println\n",
    "    }}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad91c63-1677-4ee0-9a6e-94d5df154e72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checkpoining"
    }
   },
   "outputs": [],
   "source": [
    "import com.databricks.sql.transaction.tahoe._\n",
    "\n",
    "if (!skipChecks) {\n",
    "  if (Seq(\"delta\", \"tahoe\").contains(fileFormat)) {\n",
    "    scaleFactors.foreach { scaleFactor =>\n",
    "      benchmarks.foreach{ benchmark => {\n",
    "        val dbname = getBenchmarkData(benchmark, scaleFactor)\n",
    "        sql(s\"use $dbname\")\n",
    "        time(s\"checkpointing_${benchmark}_${scaleFactor}_${fileFormat}\", {\n",
    "          sql(s\"show tables\").select(\"tableName\").collect().foreach{ row =>\n",
    "            val tableName = row.getString(0)        \n",
    "            DeltaLog.forTable(spark, tableName).checkpoint()\n",
    "          } \n",
    "        })\n",
    "        println\n",
    "      }}\n",
    "    }  \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07039a0-6a7a-4cf5-9329-d893f270b34c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Print out test times"
    }
   },
   "outputs": [],
   "source": [
    "val results = timings.toSeq.toDF(\"test\", \"time_ms\")    \n",
    "    .withColumn(\"category\", split($\"test\", \"_\")(0))\n",
    "    .select ($\"test\", $\"category\", $\"time_ms\")\n",
    "\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c74153-615d-42e0-8d4b-30bb80df2e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4077595795868347,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "TPC-DS-Datagen-Notebook",
   "widgets": {
    "catalog": {
     "currentValue": "steventan",
     "nuid": "4a1e6aa5-52ed-480d-a4ec-960e648f1755",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "catalog",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": "catalog",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "overwrite": {
     "currentValue": "true",
     "nuid": "4e4cffe2-25f8-4584-827c-e714ebc757cd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Overwrite Data",
      "name": "overwrite",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Overwrite Data",
      "name": "overwrite",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "scaleFactors": {
     "currentValue": "100",
     "nuid": "fc25b45e-bb7b-4b34-9d24-005d7b1292d5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1",
      "label": "Scale Factor",
      "name": "scaleFactors",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "1",
        "10",
        "100",
        "1000",
        "10000"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "1",
      "label": "Scale Factor",
      "name": "scaleFactors",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "1",
        "10",
        "100",
        "1000",
        "10000"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
